{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T17:29:53.540976Z",
     "start_time": "2024-12-30T17:29:49.919101Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'erpnext_issues_classification.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Filter issues where category is \"ERP Workflow\" and component is \"Accounting\"\n",
    "filtered_issues = data[\n",
    "    (data['category'] == 'ERP Workflow') &\n",
    "    (data['component'] == 'Accounting')\n",
    "].copy()\n",
    "\n",
    "# Step 2: Combine 'preconditions', 'steps_to_reproduce', 'expected_results', and 'actual_results' for embedding\n",
    "filtered_issues['combined_text'] = (\n",
    "    filtered_issues['preconditions'].fillna('') + ' ' +\n",
    "    filtered_issues['steps_to_reproduce'].fillna('') + ' ' +\n",
    "    filtered_issues['expected_results'].fillna('') + ' ' +\n",
    "    filtered_issues['actual_results'].fillna('')\n",
    ")\n",
    "\n",
    "# Check if there are at least 50 issues\n",
    "if len(filtered_issues) < 50:\n",
    "    raise ValueError(\"There are fewer than 50 issues meeting the criteria.\")\n",
    "\n",
    "# Step 3: Use Sentence-BERT for embeddings on all filtered issues\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight, fast variant of SBERT\n",
    "embeddings = model.encode(filtered_issues['combined_text'].tolist())\n",
    "\n",
    "# Step 4: Compute pairwise cosine similarity\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Step 5: Identify the top 50 most distinct issues from the entire dataset\n",
    "similarity_sums = np.sum(similarity_matrix, axis=1)\n",
    "most_distinct_indices = np.argsort(similarity_sums)[:50]\n",
    "\n",
    "# Step 6: Select the top 50 distinct issues\n",
    "distinct_issues = filtered_issues.iloc[most_distinct_indices]\n",
    "\n",
    "# Step 7: Save the results to a CSV file\n",
    "distinct_issues.to_csv('top_50_distinct_issues_sorted.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T19:30:47.287626Z",
     "start_time": "2024-12-30T19:30:00.446109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'erpnext_issues_classification.csv'  # Replace with the actual file path\n",
    "invalid_issues_file = 'erpnext_issues_all.csv'  # File containing all issues with URLs\n",
    "\n",
    "# Step 1: Load the list of invalid issues\n",
    "all_issues_data = pd.read_csv(invalid_issues_file)\n",
    "invalid_url_issues = all_issues_data[~all_issues_data['url'].str.startswith('https://github.com/frappe/erpnext/', na=False)]\n",
    "invalid_issue_numbers = set(invalid_url_issues['number'])\n",
    "\n",
    "# Step 2: Load the title data and preprocess titles\n",
    "issue_titles = all_issues_data[['number', 'title']].dropna().copy()\n",
    "\n",
    "# Preprocess titles to remove \"(backport #...)\"\n",
    "def preprocess_title(title):\n",
    "    return title.split(\"(backport\")[0].strip()\n",
    "\n",
    "issue_titles['title'] = issue_titles['title'].apply(preprocess_title)\n",
    "\n",
    "# Compute title similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight, fast variant of SBERT\n",
    "title_embeddings = model.encode(issue_titles['title'].tolist())\n",
    "title_similarity_matrix = cosine_similarity(title_embeddings)\n",
    "\n",
    "# Group issues by title similarity > 0.9\n",
    "title_threshold = 0.97\n",
    "title_groups = []\n",
    "unused_indices = set(range(len(issue_titles)))\n",
    "\n",
    "while unused_indices:\n",
    "    current = unused_indices.pop()\n",
    "    group = [current]\n",
    "    for other in list(unused_indices):\n",
    "        if title_similarity_matrix[current, other] > title_threshold:\n",
    "            group.append(other)\n",
    "            unused_indices.remove(other)\n",
    "    title_groups.append(group)\n",
    "\n",
    "# Select one issue from each title group with the longest combined_text\n",
    "selected_titles = []\n",
    "for group in title_groups:\n",
    "    group_issues = issue_titles.iloc[group].copy()\n",
    "    # Map group issues to filtered_issues by 'number'\n",
    "    group_issues_filtered = filtered_issues[filtered_issues['number'].isin(group_issues['number'])].copy()\n",
    "    if not group_issues_filtered.empty:\n",
    "        group_issues_filtered.loc[:, 'combined_length'] = (\n",
    "            group_issues_filtered['preconditions'].fillna('').apply(len) +\n",
    "            group_issues_filtered['steps_to_reproduce'].fillna('').apply(len)\n",
    "        )\n",
    "        selected_issue = group_issues_filtered.sort_values(by='combined_length', ascending=False).iloc[0]\n",
    "        selected_titles.append(selected_issue)\n",
    "\n",
    "# Update filtered_issues with the selected titles\n",
    "filtered_issues = pd.DataFrame(selected_titles)\n",
    "\n",
    "# Step 3: Combine 'preconditions', 'steps_to_reproduce', 'expected_results', and 'actual_results' for embedding\n",
    "filtered_issues['combined_text'] = (\n",
    "    filtered_issues['preconditions'].fillna('') + ' ' +\n",
    "    filtered_issues['steps_to_reproduce'].fillna('') + ' ' +\n",
    "    filtered_issues['expected_results'].fillna('') + ' ' +\n",
    "    filtered_issues['actual_results'].fillna('')\n",
    ")\n",
    "\n",
    "# Step 4: Use Sentence-BERT for embeddings on all filtered issues\n",
    "embeddings = model.encode(filtered_issues['combined_text'].tolist())\n",
    "\n",
    "# Step 5: Compute pairwise cosine similarity\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Step 6: Group issues by similarity threshold\n",
    "threshold = 0.90  # Define a similarity threshold\n",
    "groups = []\n",
    "unused_indices = set(range(len(filtered_issues)))\n",
    "\n",
    "while unused_indices:\n",
    "    current = unused_indices.pop()\n",
    "    group = [current]\n",
    "    for other in list(unused_indices):\n",
    "        if similarity_matrix[current, other] > threshold:\n",
    "            group.append(other)\n",
    "            unused_indices.remove(other)\n",
    "    groups.append(group)\n",
    "\n",
    "# Step 7: Select one issue from each group with the longest combined length\n",
    "selected_issues = []\n",
    "for group in groups:\n",
    "    group_issues = filtered_issues.iloc[group].copy()\n",
    "    group_issues.loc[:, 'combined_length'] = (\n",
    "        group_issues['preconditions'].fillna('').apply(len) +\n",
    "        group_issues['steps_to_reproduce'].fillna('').apply(len)\n",
    "    )\n",
    "    selected_issue = group_issues.sort_values(by='combined_length', ascending=False).iloc[0]\n",
    "    selected_issues.append(selected_issue)\n",
    "\n",
    "# Convert the selected issues to a DataFrame\n",
    "selected_issues_df = pd.DataFrame(selected_issues)\n",
    "\n",
    "# Step 8: Save the results to a CSV file\n",
    "selected_issues_df.to_csv('top_50_distinct_issues_sorted.csv', index=False)"
   ],
   "id": "2b9b08f2a8c74967",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscripts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mIssueSimilarityFilter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IssueSimilarityFilter\n\u001B[1;32m      3\u001B[0m filter_instance \u001B[38;5;241m=\u001B[39m IssueSimilarityFilter()\n\u001B[0;32m----> 4\u001B[0m \u001B[43mfilter_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter_issues\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43merpnext_issues_all.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtop_50_distinct_issues_sorted.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BugExploring/scripts/IssueSimilarityFilter.py:46\u001B[0m, in \u001B[0;36mIssueSimilarityFilter.filter_issues\u001B[0;34m(self, input_csv, output_csv)\u001B[0m\n\u001B[1;32m     44\u001B[0m group \u001B[38;5;241m=\u001B[39m [current]\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(unused_indices):\n\u001B[0;32m---> 46\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m title_similarity_matrix[current, other] \u001B[38;5;241m>\u001B[39m title_threshold:\n\u001B[1;32m     47\u001B[0m         group\u001B[38;5;241m.\u001B[39mappend(other)\n\u001B[1;32m     48\u001B[0m         unused_indices\u001B[38;5;241m.\u001B[39mremove(other)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T19:15:03.039021Z",
     "start_time": "2024-12-30T19:14:41.241460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pairs = []\n",
    "used_issues = set()\n",
    "n = len(filtered_issues)\n",
    "for i in range(n):\n",
    "    if filtered_issues.iloc[i]['number'] in used_issues:\n",
    "        continue\n",
    "    for j in range(i + 1, n):\n",
    "        if filtered_issues.iloc[j]['number'] in used_issues:\n",
    "            continue\n",
    "        if 0.85 <= similarity_matrix[i, j] < 0.9:\n",
    "            pairs.append((\n",
    "                filtered_issues.iloc[i]['number'],\n",
    "                filtered_issues.iloc[j]['number'],\n",
    "                filtered_issues.iloc[i]['preconditions'],\n",
    "                filtered_issues.iloc[j]['preconditions'],\n",
    "                filtered_issues.iloc[i]['steps_to_reproduce'],\n",
    "                filtered_issues.iloc[j]['steps_to_reproduce'],\n",
    "                filtered_issues.iloc[i]['expected_results'],\n",
    "                filtered_issues.iloc[j]['expected_results'],\n",
    "                filtered_issues.iloc[i]['actual_results'],\n",
    "                filtered_issues.iloc[j]['actual_results'],\n",
    "                similarity_matrix[i, j]\n",
    "            ))\n",
    "            used_issues.add(filtered_issues.iloc[i]['number'])\n",
    "            used_issues.add(filtered_issues.iloc[j]['number'])\n",
    "        if len(pairs) >= 10:\n",
    "            break\n",
    "    if len(pairs) >= 10:\n",
    "        break           \n",
    "\n",
    "# Create a DataFrame for the selected pairs\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\n",
    "    'Issue1_Number', 'Issue2_Number',\n",
    "    'Issue1_Preconditions', 'Issue2_Preconditions',\n",
    "    'Issue1_Steps', 'Issue2_Steps',\n",
    "    'Issue1_Expected', 'Issue2_Expected',\n",
    "    'Issue1_Actual', 'Issue2_Actual', 'Similarity'\n",
    "])\n",
    "\n",
    "# Save the pairs to a CSV file\n",
    "pairs_df.to_csv('similarity_pairs_0.85_0.9.csv', index=False)\n"
   ],
   "id": "794cc06de054962b",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T19:09:08.748343Z",
     "start_time": "2024-12-30T19:08:13.845288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title_pairs = []\n",
    "used_issues = set()\n",
    "n = len(issue_titles)\n",
    "for i in range(n):\n",
    "    if issue_titles.iloc[i]['number'] in used_issues:\n",
    "        continue\n",
    "    for j in range(i + 1, n):\n",
    "        if issue_titles.iloc[j]['number'] in used_issues:\n",
    "            continue\n",
    "        if 0.95 <= title_similarity_matrix[i, j] < 0.98:\n",
    "            title_pairs.append((\n",
    "                issue_titles.iloc[i]['number'],\n",
    "                issue_titles.iloc[j]['number'],\n",
    "                issue_titles.iloc[i]['title'],\n",
    "                issue_titles.iloc[j]['title'],\n",
    "                title_similarity_matrix[i, j]\n",
    "            ))\n",
    "            used_issues.add(issue_titles.iloc[i]['number'])\n",
    "            used_issues.add(issue_titles.iloc[j]['number'])\n",
    "        if len(title_pairs) >= 10:\n",
    "            break\n",
    "    if len(title_pairs) >= 10:\n",
    "        break\n",
    "\n",
    "# Create a DataFrame for the selected title pairs\n",
    "title_pairs_df = pd.DataFrame(title_pairs, columns=[\n",
    "    'Issue1_Number', 'Issue2_Number', 'Issue1_Title', 'Issue2_Title', 'Similarity'\n",
    "])\n",
    "\n",
    "# Save the title pairs to a CSV file\n",
    "title_pairs_df.to_csv('title_similarity_pairs_0.95_0.98.csv', index=False)"
   ],
   "id": "eeb742add666b71",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T19:47:23.826524Z",
     "start_time": "2024-12-30T19:46:54.531528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.analysis.issue_filter import IssueSimilarityFilter\n",
    "\n",
    "filter_instance = IssueSimilarityFilter()\n",
    "filter_instance.filter_issues('erpnext_issues_classification.csv', 'erpnext_issues_all.csv', 'filtered_issues_title.csv')"
   ],
   "id": "14ea061dc3d1120f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linsen/BugExploring/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
