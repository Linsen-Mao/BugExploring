{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:17:07.594714Z",
     "start_time": "2024-12-28T17:15:55.931805Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import openai  # Import OpenAI for LLM analysis\n",
    "import json\n",
    "\n",
    "# GitHub API base URL for issues\n",
    "BASE_URL = \"https://api.github.com/repos/frappe/erpnext/issues\"\n",
    "\n",
    "# Headers for GitHub API (optional: include your personal token for higher rate limits)\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    \"Authorization\": \"token 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\"  # Use GitHub token\n",
    "}\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Function to fetch issues and pull requests with a limit on the number of items\n",
    "def fetch_issues_with_limit(limit):\n",
    "    issues = []\n",
    "    page = 1\n",
    "    fetched_count = 0\n",
    "\n",
    "    while fetched_count < limit:\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        params = {\n",
    "            \"state\": \"all\",  # Fetch open and closed issues and pull requests\n",
    "            \"per_page\": 100,\n",
    "            \"page\": page,\n",
    "        }\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:  # Exit loop if no more issues are found\n",
    "            break\n",
    "\n",
    "        remaining = limit - fetched_count\n",
    "        issues.extend(data[:remaining])\n",
    "        fetched_count += len(data[:remaining])\n",
    "\n",
    "        if fetched_count >= limit:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return issues\n",
    "\n",
    "# Function to classify an issue or pull request using LLM\n",
    "\n",
    "def classify_issue_and_analyze(title, body, is_pull_request):\n",
    "\n",
    "    if not title or not body:\n",
    "        return {\n",
    "            \"category\": \"Insufficient information\",\n",
    "            \"reason\": \"Insufficient information provided in title or description.\",\n",
    "            \"preconditions\": \"\",\n",
    "            \"steps_to_reproduce\": \"\",\n",
    "            \"expected_results\": \"\",\n",
    "            \"actual_results\": \"\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    categories = [\n",
    "        \"UI/UX Issues\",\n",
    "        \"Workflow Issues\",\n",
    "        \"Performance and Compatibility Issues\",\n",
    "        \"Documentation and Validation Issues\",\n",
    "        \"Security Issues\",\n",
    "        \"Other (Miscellaneous)\"\n",
    "    ]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "  Classify the following {'pull request' if is_pull_request else 'issue'} into one of these categories and extract the following fields if present. If not present, leave the field blank:\n",
    "  \n",
    "  - UI/UX Issues: Problems related to graphical user interface changes or issues that do not affect business logic or database data. This includes layout issues, interaction design problems, and usability enhancements.\n",
    "  - Workflow Issues: Problems that may affect business logic or database data, including errors in workflows, data processing anomalies, or module interaction conflicts. This covers core ERP processes such as financial management, inventory control, and order processing.\n",
    "  - Performance and Compatibility Issues: System performance issues, such as slow response times, resource inefficiency, or integration with third-party systems and module compatibility.\n",
    "  - Documentation and Validation Issues: Missing or incorrect documentation, insufficient testing coverage, or validation logic errors.\n",
    "  - Security Issues: Vulnerabilities, such as data leaks, insufficient encryption, or improper access controls.\n",
    "  - Other (Miscellaneous): Minor inconsistencies or rare edge cases that do not fit into the above categories.\n",
    "  \n",
    "  {'If this pull request addresses an issue, classify it based on the issue it resolves. If it introduces a new feature or enhancement, classify it based on the relevant category.' if is_pull_request else ''}\n",
    "  \n",
    "  Extract the following fields if they exist:\n",
    "  - preconditions\n",
    "  - steps_to_reproduce\n",
    "  - expected_results\n",
    "  - actual_results\n",
    "  \n",
    "  Output the result in strict JSON format with the following structure:\n",
    "  {{\n",
    "    \"category\": \"<category>\",\n",
    "    \"reason\": \"<reason>\",\n",
    "    \"preconditions\": \"<preconditions>\",\n",
    "    \"steps_to_reproduce\": \"<steps_to_reproduce>\",\n",
    "    \"expected_results\": \"<expected_results>\",\n",
    "    \"actual_results\": \"<actual_results>\"\n",
    "  }}\n",
    "  \n",
    "  Title: {title}\n",
    "  Description: {body}\n",
    "  \"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that classifies issues into predefined categories and extracts specific fields.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # Try to parse JSON, clean up if necessary\n",
    "    try:\n",
    "        # Ensure the content starts and ends with valid JSON\n",
    "        content = content.strip().lstrip('```json').rstrip('```').strip()\n",
    "        result = json.loads(content)\n",
    "        category = result.get(\"category\", \"Other (Miscellaneous)\")\n",
    "        reason = result.get(\"reason\", \"No explanation provided.\")\n",
    "        preconditions = result.get(\"preconditions\", \"\")\n",
    "        steps_to_reproduce = result.get(\"steps_to_reproduce\", \"\")\n",
    "        expected_results = result.get(\"expected_results\", \"\")\n",
    "        actual_results = result.get(\"actual_results\", \"\")\n",
    "        labels = result.get(\"labels\", [])\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle parsing failure gracefully\n",
    "        category = \"Other (Miscellaneous)\"\n",
    "        reason = \"Could not parse the JSON response. Raw content: \" + content\n",
    "        preconditions = \"\"\n",
    "        steps_to_reproduce = \"\"\n",
    "        expected_results = \"\"\n",
    "        actual_results = \"\"\n",
    "        labels = []\n",
    "\n",
    "    # Validate the category\n",
    "    if category not in categories:\n",
    "        category = \"Other (Miscellaneous)\"\n",
    "        reason = \"Category not recognized, defaulting to Other.\"\n",
    "\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"reason\": reason,\n",
    "        \"preconditions\": preconditions,\n",
    "        \"steps_to_reproduce\": steps_to_reproduce,\n",
    "        \"expected_results\": expected_results,\n",
    "        \"actual_results\": actual_results,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Parse fetched issues into a Pandas DataFrame\n",
    "def parse_issues_to_dataframe(issues):\n",
    "    all_data = []\n",
    "    classifications = []\n",
    "\n",
    "    for issue in issues:\n",
    "        is_pull_request = \"pull_request\" in issue  # Check if it's a pull request\n",
    "        labels = [label[\"name\"] for label in issue.get(\"labels\", [])]  # Extract labels\n",
    "        issue_data = {\n",
    "            \"id\": issue.get(\"id\"),\n",
    "            \"number\": issue.get(\"number\"),\n",
    "            \"title\": issue.get(\"title\"),\n",
    "            \"state\": issue.get(\"state\"),\n",
    "            \"created_at\": issue.get(\"created_at\"),\n",
    "            \"updated_at\": issue.get(\"updated_at\"),\n",
    "            \"closed_at\": issue.get(\"closed_at\"),\n",
    "            \"url\": issue.get(\"html_url\"),\n",
    "            \"is_pull_request\": is_pull_request,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        all_data.append(issue_data)\n",
    "\n",
    "        # Classify the issue or pull request and analyze specific fields\n",
    "        analysis = classify_issue_and_analyze(issue.get(\"title\"), issue.get(\"body\", \"\"), is_pull_request)\n",
    "        classifications.append({\n",
    "            \"number\": issue.get(\"number\"),\n",
    "            \"category\": analysis[\"category\"],\n",
    "            \"reason\": analysis[\"reason\"],\n",
    "            \"preconditions\": analysis[\"preconditions\"],\n",
    "            \"steps_to_reproduce\": analysis[\"steps_to_reproduce\"],\n",
    "            \"expected_results\": analysis[\"expected_results\"],\n",
    "            \"actual_results\": analysis[\"actual_results\"],\n",
    "            \"labels\": \", \".join(labels)  # Join labels into a string for CSV\n",
    "        })\n",
    "\n",
    "    df_all = pd.DataFrame(all_data)\n",
    "    df_classification = pd.DataFrame(classifications)\n",
    "\n",
    "    df_url = df_all[[\"url\"]]\n",
    "    df_basic = df_all[[\"url\", \"number\", \"state\"]]\n",
    "\n",
    "    return df_url, df_basic, df_all, df_classification\n",
    "\n",
    "# Main function to fetch and save the data\n",
    "def main(limit=50):\n",
    "    print(f\"Fetching up to {limit} issues and pull requests from ERPNext repository...\")\n",
    "    issues = fetch_issues_with_limit(limit)\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"Fetched {len(issues)} issues and pull requests.\")\n",
    "        df_url, df_basic, df_all, df_classification = parse_issues_to_dataframe(issues)\n",
    "        \n",
    "        # Save to CSV files\n",
    "        df_url.to_csv(\"erpnext_issues_url.csv\", index=False)\n",
    "        print(\"URL-only issues saved to 'erpnext_issues_url.csv'.\")\n",
    "        \n",
    "        df_basic.to_csv(\"erpnext_issues_basic.csv\", index=False)\n",
    "        print(\"Basic issues saved to 'erpnext_issues_basic.csv'.\")\n",
    "        \n",
    "        df_all.to_csv(\"erpnext_issues_all.csv\", index=False)\n",
    "        print(\"All issues saved to 'erpnext_issues_all.csv'.\")\n",
    "\n",
    "        df_classification.to_csv(\"erpnext_issues_classification.csv\", index=False)\n",
    "        print(\"Classified issues saved to 'erpnext_issues_classification.csv'.\")\n",
    "    else:\n",
    "        print(\"No issues found or an error occurred.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(limit=50)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 50 issues and pull requests from ERPNext repository...\n",
      "Fetching page 1...\n",
      "Fetched 50 issues and pull requests.\n",
      "URL-only issues saved to 'erpnext_issues_url.csv'.\n",
      "Basic issues saved to 'erpnext_issues_basic.csv'.\n",
      "All issues saved to 'erpnext_issues_all.csv'.\n",
      "Classified issues saved to 'erpnext_issues_classification.csv'.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e95806a0c485b235"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
